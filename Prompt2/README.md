# Prompt 2 / Data Prompt 2
## IoT Data Oracle

### Background
With the advancement of technology, the number of energy generation sources is rapidly growing; from utility scale wind and solar farms, to commercial, industrial and residential households being able to generate their own electricity using solar PV and increasingly battery storage too. Given such distributed energy resources (DER) centralized tracking of generation and clearance is no longer feasible at scale. At the same time, new technological developments in Blockchain, IoT, remote sensing (such as satellites) and Data Science create the opportunity to achieve consensus, tracking and transparency on the amount of electricity generated and emissions avoided by DER, which is the necessary foundation for trading energy and Renewable Energy Credits from DER, as well as many other applications such as carbon offset tracking.

### Challenge
Create a Data Validation Oracle, for reaching a consensus on the ‘true’ or statistically wighted amount of energy generated by a distributed energy resource, such as rooftop solar. This could include a combination of IoT, environmental, remote sensing and benchmark data to create the most robust mechanism to evaluate data streams, create device reputation scores, detect anomalies and ultimately output a consensus result. Possible approach to use machine learning and other advanced anomaly detection algorithms.

Here is a potential list of components and steps you might take in approaching this challenge.
Feel free to do only parts of it, work together with other teams to bring together different parts or do a completely different approach altogether. Document findings and ideas along the way, as these are just as valuable as your final code (for example any data you would ideally need to best solve this - even if you couldn’t find it).

Data input:
- Think about what Data you would ideally need
- Assess the data feeds available
- Find additional data feeds

Data pipeline:
- Build a pipeline that brings together all data feeds into a central database and can handle the data in real time
- Structure your data & storage in a way that makes it easy to connect data of different time and geographic resolutions to your assets/analysis and is efficient

Data Analysis:
- Consider what your final output will be. Will it be a single number of kWh electricity generated? Or will it have additional attributes like a confidence score? Or will it be probability distribution?
- What is the timescale of the data inputs you have? (E.g. if you only get solar irradiance data in 10 minute intervals, but panel output data in 1 minute intervals, you might need ‘wait’ until the irradiance data comes in before making any analysis)
- Build the mathematical calculation logic. How do you validate the data from the local output meter on the panel?
- Process data as soon as possible

Data output
- Create an output data feed that should be the ‘actual generation’
- This can be a single number or an object with multiple characteristics as per 3.a


## Data Requirement and Sources:
We can do this for either a solar or a wind project (or if possible both). Solar is more suitable as it is often small scale and harder to track.

Solar:
Solar generation data
1 year at hourly resolution
At least 3 measurements (sections of one installation) or separate installations within immediate proximity for benchmarking, ideally 10+
Satellite data:
Solar irradiance
Same year at hourly resolution
Ground weather station data
Temperature, solar irradiance
Same year at hourly resolution or better
Data irregularities that we want the algorithm to be able to detect
Real or artificially created
Multiple different types?
If we don’t/can’t provide these, we need to give students good descriptions of what anomalous data might look like, so they can generate it themselves (build the chaos monkey into their system themselves)
Technical details of the solar panel installed
Rated capacity
Current - power - Voltage curve ideally for various temperatures, e.g.:


For all of this data, static, past data is great as a first step, even better would be to have real data feeds, so students can work on real time integration and assessment. The frequency and density of data mentioned is an indication of what the minimum we would need is, but ideally it should reflect what is actually typically collected in the industry. For example if data is available at minute resolution we shouldn’t aggregate it, but instead use the real data feed and let students make best use of it.

Wind:
Wind generation data
1 year at hourly resolution
At least 3 turbines within immediate proximity for benchmarking, ideally 10+
Ground weather station data
Wind speeds
Wind direction
Same year at hourly resolution or better

Prompt owner and technical mentors:
Yale Openlab
Mentors: Daniel Csonth, Martin Wainstein, Swytch.io?, (WattTime/EW?)
